# Chapter 1.3
## Multiprocessor systems
Systems can have multiple CPU cores, or multiple processors. This can make the system process faster as work can be shared between CPUs. It does not scale linearly, as there is overhead in memory management complexity. Namely, the system bus interconnect has to manage communication between cores and shared memory. An optimazation is to give each core its own cache and fast memory, and use clever optimizations by the operating system on how to structure and load memory. Symmetric multiprocessing means all cores perform all tasks. Multicore designs that put more cores on the same chip are more common and efficient.

## Clustered systems
Clustered systems consist of multiple seperate loosely coupled nodes that are typically themselves multicore systems. This is useful for high-availability services where a node can step in and pck up the work of a failed node. This can be done with hot spares(assymmetric clustering), but using a system of mutual monitoring of several nodes(symmetric clustering) is more efficient as there are no machines sitting around doing nothing. A system that keeps functionality proportional to the amount of functioning hardware is said to have **graceful degredation**. **Fault tolerance** sets a bar for how much of the system can fail while keeping _full_ functionality. Software needs to be made to be run on a clustering, usually meaning a high degree of paralelization.

### Distributed databases
These require special software and a **distributed lock manager**.